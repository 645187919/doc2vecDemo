# doc2vecDemo
工作中写的一个Demo，应用场景：如智能客服中对输入的问题进行相似问题的匹配。将这个Demo加到工程中可以减小后期工程维护的工作量（只要有新的问题可以直接放到对应的questions.txt文档中生成一个新的model然后就可以匹配新的问题）。

**开发工具：Pycharm+Python3.6+jieba+gensim**

程序中有详细的注释，如不能为您解惑，请留言。
**这里再详细介绍下程序中一些参数的主要含义以及如何初步调参的操作：**

-cbow 0表示不使用cbow模型，默认为Skip-Gram模型

-size 表示词向量维数：经验是不超过1000，超过1000基本都没有效果，我感觉是可以取  sqrt(dict_size)/2 的样子，不过最好多试几个

-window 上下文窗口，是训练词向量的时候，取上下文的大小，感觉这个一般都是5比较好

-sample 表示采样参数，是一个经验忽视掉频率过高的词的参数，一般也用默认的就行。

-sampe指的是采样的阈值，如果一个词语在训练样本中出现的频率越大，那么就越会被采样。

-negative 表示负采样参数，每有一个正样本，选择几个负样本的参数，一般也是用默认的。

-binary为1指的是结果二进制存储，为0以文本形式进行存储。

上面这两个参数试过很多个，感觉就是默认的比较好用。
-hs 做不做层次的softmax，是1的时候效果比较好，但是训练会慢一点

-min-count 控制词典大小的参数，如果只想要高频词的话就设置高一点。

架构：
skip-gram（慢、对罕见字有利）vs CBOW（快）

训练算法：分层softmax（对罕见字有利）vs 负采样（对常见词和低纬向量有利）

欠采样频繁词：可以提高结果的准确性和速度（适用范围1e-3到1e-5）

文本（window）大小：skip-gram通常在10附近，CBOW通常在5附近

**关于Doc2Vec的原理以及其他的一些详细介绍请参考我的博客：**

https://blog.csdn.net/qq_16633405/article/details/80227805

https://blog.csdn.net/qq_16633405/article/details/80480300
